---
title: "Group Project on Financial Data"
output:
  pdf_document: default
---

```{r}
library(tidyverse)
library(readr)
library(quadprog)
set.seed(123)
```


# Introduction
In this project, we work with financial time-series data, specifically, the prices of stocks in the S&P 500 index to obtain an "optimal" portfolio of stocks.

# Dataset
We obtain a dataset of Stock prices from the Standard and Poor's 500 (S&P 500) index through Yahoo Finance. We used the time period of 2020-01-02 to 2021-01-02, and utilise a total of 495 stock prices.

```{r}
# Load the data from a CSV file
stock_prices <- read_csv("./data/sp500_stock_data.csv")

# Convert Date to a Date object if it's not already
stock_prices$Date <- as.Date(stock_prices$Date)
```

# Markowitz' Mean-Variance Model
<! https://sites.math.washington.edu/~burke/crs/408/fin-proj/mark1.pdf --> 

We define the **return** of an asset (our stock) which has price $P_t$ at time $t$ and price $P_{t-1}$ at time $t-1$ as being $R_t = \frac{P_t}{P_{t-1}}$. 

The well-known **Markowitzâ€™s Mean-Variance Optimisation** is the basis of our Portfolio strategy. In this setting, the return on assets are modelled as random variables, and the goal is to choose a portfolio of **weighting factors** through an optimality criterion. Specifically, we have $n$ stocks which we weight in our portfolio with a set of weighting factors $\{w_i\}_{i=1}^p$. The idea then is to maximize the expected return and to minimize the volatility at the same time. Mathematically speaking, we aim to maximize \[\mathbb{E}[R] = \sum_{i}w_i\mathbb{E}[R_i] \] 
subject to minimize\[\sigma^2 = \sum_{i,j}w_iw_j\sigma_i \sigma_j \rho_{ij}\]
where $\{R_i\}_i$ is the percentage return on the underlying assets; $\{w_i\}_i$ is the respective proportion that sum to 1; $\{\sigma_i\}$ is the standard deviation of the return on the $i$th underlying asset and $\rho_{ij}$ is the correlation between $i$th return and $j$th return.

## Portfolio Strategy
We wish to get estimates of the above objects, and since our data is time series and therefore not independent and identically distributed, we will update our estimates sequentially. Also, since there are too many (in our case 500 stocks) variables, we will use dimension reduction techniques. The reduced factors could also vary from time to time. Combined these observations, we do the following strategy:

1. Once we decide to reallocate the portfolio upon meeting some predefined general criteria $C$, do
2. Update the reduced factors where each factor is a linear combination of the underlying stocks. 
3. Update the estimates of expectation and covariance between reduced factors.
4. Get the new weights $\{w_i\}$ by maximizing $\mathbb{E}[R]$ subject to minimize $\sigma^2$ and weights sum to 1.


# Principal Components Analysis
We will use Principal Component Analysis(PCA) as a dimension reduction technique.
```{r}
# Load the data from a CSV file
stock_prices <- read_csv("./data/sp500_stock_data.csv")

# Convert Date to a Date object if it's not already
stock_prices$Date <- as.Date(stock_prices$Date)

# Using all of 2020
start_date <- as.Date("2020-01-02")
end_date <- as.Date("2020-12-31")

# Filter for specific date range and select only Date, Close, and Ticker
stock_prices_filtered <- stock_prices %>%
  filter(Date >=  start_date & Date <= end_date) %>%
  select(Date, Close, Ticker)

# Reshape data to a wide format
wide_data <- stock_prices_filtered %>%
  pivot_wider(names_from = Ticker, values_from = Close)

wide_data <- wide_data[, !apply(is.na(wide_data), 2, any)]

#Looking at 10 days
start_day <- 100
end_day <- 110

# Calculate logarithmic returns
log_returns_monthly <- wide_data[start_day:end_day, ] %>%
  mutate(across(-Date, ~log(. / .[1]) + 1)) %>%
  select(-Date) 

log_returns_monthly
```

```{r}
# Perform PCA on logarithmic returns
pca_result <- prcomp(log_returns_monthly, scale = TRUE, center= TRUE)


explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)
cumulative_variance <- cumsum(explained_variance)

# Find the number of components that explain at least the threshold variance
num_components <- which(cumulative_variance >= 0.9)[1]
print(num_components)

# Return the principal components
y <- pca_result$x[, 1:num_components]
```

# State-space Model and Gaussian Process Regression(GPR) Modelling
As mentioned in the previous sections, we do PCA first to get independent components. In this section we will try to predict the market value change in short future. Each components' behaviour is viewed as independent time series, and we will fit each time series with the simple linear state-space model:$$X_t = \phi X_{t-1} + V$$ $$Y_t = X_t + W$$ where
$W\sim \mathcal{N}(0,\sigma_w^2)$ and $V\sim \mathcal{N}(0,\sigma_v^2)$.

## Kalman Filter
Note that using Kalman Filter with initiate distribution $\mathcal{N}(0,1)$ is equivalent to a Gaussian process regression with the following relation:
$\phi = \exp(-\frac{1}{\gamma})$ and
$\sigma_v^2 = 1-\exp(-\frac{2}{\gamma})$.

The `Kalman` function takes inputs $\phi$, $\sigma_v^2$, $\sigma_w^2$,
$m_0$, $\sigma_0^2$ and the number of prediction days $n$, it returns a the predicted mean, predicted
variance, updated mean and updated variance.
It is a general Kalman Filter funtion and will be then implemented using our relation to GPR.
```{r}
kalman <- function(y,phi,Sigmav,Sigmaw,m0,Sigma0,n=2){
  
  T <- length(y)
  
  #initialization
  mu.p <- rep(NA,T+n)
  Sigma.p <- rep(NA,T+n)
  mu.f <- rep(NA,T)
  Sigma.f <- rep(NA,T)


  
  #forward recursion time1
  mu.p[1] <- m0
  Sigma.p[1] <- Sigma0
  mu.f[1] <- m0 + (y[1]-m0)*(Sigma0/(Sigma0+Sigmaw))
  Sigma.f[1] <- Sigma0-(Sigma0^2/(Sigma0+Sigmaw))

  #forward recursion time 2:T
  for (t in 2:T){
    
    #prediction
    mu.p[t] <- phi*mu.f[t-1]
    Sigma.p[t] <- phi^2 * Sigma.f[t-1] + Sigmaw
    
    #update
    deno <- Sigmaw + Sigma.p[t]
    mu.f[t] <- Sigmaw*mu.p[t]/deno + Sigma.p[t]*y[t]/deno
    Sigma.f[t] <- Sigmaw*Sigma.p[t]/deno
  }
  #predict for T+1:T+n
  for (t in (T+1):(T+n)){
    if (t == T+1){
      mu.p[t] <- phi*mu.f[t-1]
      Sigma.p[t] <- phi^2 * Sigma.f[t-1] + Sigmaw
    }
    else{
      mu.p[t] <- phi*mu.p[t-1]
      Sigma.p[t] <- phi^2 * Sigma.p[t-1] + Sigmaw
    }
  }
  return (list(mu.f=mu.f,Sigma.f=Sigma.f,mu.p=mu.p,Sigma.p=Sigma.p))
}
```

We then implement it with GPR:
```{r}
kf.gp <- function(y,gamma,Sigmaw,m0=0,Sigma0=1,n=2){
  T=length(y)
  #update Sigmav and phi
  phi <- exp(-1/gamma)
  Sigmav <- 1-exp(-2/gamma)
  result <- kalman(y,phi=phi,Sigmav=Sigmav,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0,n)
  
  return (list(mu.p=result$mu.p, Sigma.p=result$Sigma.p,
               mu.f=result$mu.f, Sigma.f=result$Sigma.f))
  
}

kf.loglikelihood1 <- function(y,mu.p,Sigma.p,Sigmaw,m0=0,Sigma0=1){
  T <- length(y)
  likelihood <- rep(NA,T)
  
  #at time 1
  likelihood[1] <- log(dnorm(y[1],mean=m0,sd = sqrt(Sigma0 + Sigmaw)))
  
  #time 2:T
  for (t in 2:T){
    likelihood[t] <- log(dnorm(y[t],mean=mu.p[t],sd=sqrt(Sigmaw+Sigma.p[t])))
  }
  return (sum(likelihood))
}
```

There are two functions here: $\bullet$ `kf.gp` simply applies the
Kalman Filter on observed $y$, with $\sigma_v^2$ and $\phi$ as functions
of hyper parameter $\gamma$, computing the predictive and updated distributions. $\bullet$ `kf.loglikelihood` computes the
loglikelihood of the observed $y$ in a iteration manner by noting
$$\log(p(y_{1:T})) = p(y_1) + \sum_{t=2}^T p(y_t|y_{1:t-1})$$ and
$$p(y_t|y_{1:t-1}) = \mathcal{N}(y_t;m_{t|t-1},\sigma_{t|t-1}^2 + \sigma_w^2)$$

## Optimization to get hyperparameters' MLE
In real life we never observe the hyper parameters $\gamma$ and
$\sigma_w^2$. We propose using `optim` to optimize against $\sigma_w^2$ and $\gamma$
against the loglikehood computed using `kf.loglikelihood`.

```{r}
kf.loglikelihood <- function(y,gamma,Sigmaw,m0=0,Sigma0=1){
  o <- kf.gp(y=y,gamma=gamma,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0)
  mu.p <- o$mu.p
  Sigma.p <- o$Sigma.p
  result <- kf.loglikelihood1(y=y,mu.p=mu.p,Sigma.p=Sigma.p,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0)
  return (result)
}

optim_parm <- function(y){
  opt_param <- optim(par = c(5,0.5), 
                     fn = function(parm) -1*kf.loglikelihood(y,parm[1],  parm[2]))
  return(list(gamma = opt_param$par[1], 
              Sigmaw=opt_param$par[2]))
}
```

## Model fitting

```{r}
#Only predict 1 day

# Initialize an empty dataframe to store the time series data
time_series_df <- data.frame(matrix(ncol = 2, nrow = num_components))
rownames(time_series_df) <- paste("Component", 1:num_components, sep = "")
colnames(time_series_df)[1] <- "mu"
colnames(time_series_df)[2] <- "sigma2"


for (m in 1:num_components) {
  # Obtain optimized hyperparameters for the m-th component
  optim_hyperparam <- optim_parm(y[, m])

  # Run the Gaussian Process with Kalman filter
  gp_result <- kf.gp(y[, m], optim_hyperparam$gamma, optim_hyperparam$Sigmaw, n = 0)
  
  # Add the time series to the dataframe
  time_series_df[m, ] <- c(tail(gp_result$mu.p, 1) , tail(gp_result$Sigma.p, 1))
}

time_series_df
```

Readjust the predicted cumulative returns ($r_{T+1} =1 + \log \frac{P_{T+1}}{P_{0}} = 1 + \log(P_{T+1}) - \log(P_0)$ ) as follows:
$$
z_{T+1 = } \log \frac{P_{T+1}}{P_T} = \log(P_{T+1}) - \log(P_T) - \log(P_0) + \log(P_0) = r_{T+1} - r_{T}  
$$
As we use the predicted daily log returns $z_{T+1}$ for our portfolio optimisation.

```{r}
pred_returns <- time_series_df$mu - tail(y,1) #z_T+1
#covar_mat <- diag(pca_result$sdev[1:num_components] ** 2)
covar_mat <- diag(time_series_df[[2]])
Amat <- matrix(0, num_components, num_components)
Amat[,1] <- 1
bvec <- rep(0,num_components)
bvec[1] <- 1
lambda <- 1

QP_result <- solve.QP(2*covar_mat, lambda * pred_returns, matrix(1, nrow=num_components,ncol=1), c(1), meq=1)
QP_result$solution


```

```{r}
y = wide_data$AAPL
yy <- c(1)
for (i in 2:length(y)){
  yy <- c(yy,1+log(y[i]/y[1]))
}

optim_hyperparam = optim_parm(yy)
results = kf.gp(yy,gamma=optim_hyperparam$gamma,Sigmaw = optim_hyperparam$Sigmaw,n=0)
mu.p <- results$mu.p
Sigma.p <- results$Sigma.p

se.p <- sqrt(Sigma.p)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.p <- mu.p + cv99*se.p
CIlower.p <- mu.p - cv99*se.p
time = 1:(length(yy)+1)
yy <- c(yy,yy[length(yy)])
plot(time,yy,cex=0.5,col='darkgreen',pch=5,ylim=c(.9,1.1),main='Predicted y and observed y')
points(time,mu.p,cex=0.5,col='red',pch=10)
points(time,CIupper.p,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.p,col='blue',type ='l',lty=2,lwd=1)
legend(1,.95,legend= c('Observation','Predicted','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.6)
```
